{
  "title": "Как развернуть простой Kubernetes-кластер с VictoriaMetrics? Пособие для начинающих",
  "views": "690",
  "link": "https://habr.com/ru/companies/selectel/articles/934530/",
  "text": {
    "article_content": [
      "Развертывание Kubernetes-кластера и системы мониторинга часто воспринимается как сложная задача, которая требует глубоких знаний и значительных временных затрат. Однако современные инструменты автоматизации позволяют существенно упростить этот процесс, поэтому разобраться смогут и начинающие специалисты.",
      "Привет, Хабр! Меня зовут Катя Низовцева, я системный администраторв Selectel. В этой статье мы подробно рассмотрим, как с помощью Kubespray быстро и эффективно развернуть работоспособный Kubernetes-кластер, а также интегрировать с ним систему мониторинга VictoriaMetrics. Этот подход особенно полезен, когда необходимо оперативно создать тестовое окружение или подготовить базовую инфраструктуру для дальнейшего развития.",
      "Представленная инструкция охватывает все ключевые этапы — от создания виртуальных машин до проверки сбора метрик. Полученная в результате конфигурация может служить надежной основой для более сложных решений или использоваться в качестве учебного стенда для освоения Kubernetes и систем мониторинга.",
      "Вы узнаете, как подготовить инфраструктуру для развертывания, какие настройки Kubespray требуют особого внимания, а также — как правильно установить и настроить VictoriaMetrics. Все это будет полезно системным администраторам, осваивающим контейнерные технологии, DevOps-инженерам, которые ищут способы ускорения развертывания инфраструктуры, и разработчикам.",
      "Все шаги сопровождаются подробными объяснениями и практическими рекомендациями, что позволит избежать типичных ошибок и получить рабочую систему в кратчайшие сроки.",
      "Kubespray — это набор готовых сценариев (playbooks) для Ansible, которые автоматизируют установку и настройку Kubernetes-кластера. Вместо того чтобы вручную настраивать каждую ноду, вы описываете конфигурацию в файлах, а Kubespray делает всю работу за вас.",
      "Что мы здесь видим?Первое — вспомогательную ВМ, на которой у нас будет установлен Ansible. Это может быть также ваш локальный хост. Тестовый кластер же будет состоять из одной мастер-ноды и воркер-ноды.",
      "Мастер-нода (control plane) — это «мозг» кластера, управляющий всеми процессами. Она отвечает за принятие решений о размещении подов, отслеживание состояния кластера, обработку API-запросов и хранение конфигурации в etcd.",
      "Воркер-нода — это нода, на которой непосредственно запускаются ваши приложения (поды).",
      "Такая минимальная конфигурация из мастер- и воркер-ноды идеально подходит для тестирования, разработки и изучения Kubernetes. При этом сохраняются все ключевые функции полноценного кластера. В продакшен-среде рекомендуется использовать несколько мастеров для отказоустойчивости по алгоритму RAFT.",
      "Предварительно подготовим инфраструктурув панели управления.",
      "Облачный сервер с криперами и порталом в Незер.Добывайте ресурсы, стройте объекты, исследуйте мир Selectel в Minecraft и получайте призы.",
      "Исследовать →",
      "Создаем новую приватную сеть в интересующем пуле, с интересующим CIDR. Эта подсеть будет использоваться для нод, входящих в состав кластера. Для этого переходим во вкладкуПродукты → Облачная платформа → Облачные серверы → Сеть → Приватные сети.",
      "Внимание!DHCP лучше выключить, так как по истечению DHCP-lease IP-адрес с ноды может пропасть, что повлечет за собой проблемы в сети.",
      "Отлично. Далее создаем облачный роутер в том же пуле и подключаем его к внешней сети. Это можно сделать во вкладкеПродукты → Облачная платформа → Облачные серверы → Сеть → Облачные роутеры.",
      "Затем подключаем ранее созданную приватную сеть к этому роутеру.",
      "Далее перейдем во вкладкуПродукты → Облачная платформа → Облачные серверыи нажмем кнопкуСоздать сервер.",
      "На всех ВМ в качестве ОС выбрана Ubuntu 24.04. Используемые конфигурации для ВМ под разные функциональности описаны ниже. При этом они именно рекомендуемые для тестовой среды, но могут быть изменены в зависимости от нагрузки.",
      "Виртуальная машина с Ansible:1 vCPU, 2 ГБ RAM, 20 ГБ диска.",
      "Мастер-нода кластера:2 vCPU, 4 ГБ RAM, 20 ГБ диска.",
      "Воркер-нода кластера:1 vCPU, 2 ГБ RAM, 10 ГБ диска.",
      "В качестве приватной сети выбираем созданную ранее нами подсеть.",
      "Ниже — пример создания мастер-ноды:",
      "В разделеДоступдобавляем свой SSH-ключ, так как вместе с Ubuntu 24.04 LTS 64-bit необходимо использовать SSH-ключ.",
      "По аналогии повторяем процесс для воркер-ноды (kubernetes-worker) и ВМ с Ansible (ansible).",
      "О безопасности.По умолчанию ВМ получает публичный IP, но его можно заменить на доступ через облачный роутер с опциональным файрволом. Файрвол — это важный шаг для защиты кластера. Подробнее о межсетевом экране на облачном роутере можно почитатьв документации.",
      "1. Для начала нам нужно обновить и установить необходимые пакеты, а также сгенерировать SSH-ключ для безопасного подключения к нодам.",
      "2. На хостах будущего Kubernetes-кластера добавляем ключ этой ВМ в авторизованные:",
      "3. Далее снова переходим к ВМ с Ansible, клонируем официальный репозиторий Kubepsray.",
      "Внимание! Версия Kubespray может меняться — стоит проверить актуальный стабильный релизна GitHub.",
      "4. Зададим переменные окружения, создадим и запустим виртуальное окружение Python, а также установим необходимые зависимости для проекта, которые указаны в файле requirements.txt.",
      "С помощью виртуального окружения зависимости проекта не конфликтуют с глобальными пакетами системы.",
      "Например declare -a IPS=(10.233.22.2 10.233.22.3) указывают на IP из той же подсети, где живет ВМ с Ansible-хост.",
      "Примечание.Если же вы используете хост в другой подсети, то вам необходимо обеспечить сетевую связность между ВМ c Ansible и будущими нодами вашего Kubernetes-кластера. Если ВМ находятся в одном пуле, то достаточно будет эти две сетиподключить к одному облачному роутеру. А если ВМ будут находиться  в разных пулах, то связность обеспечивается черезглобальный роутер.",
      "Далее заполняем файл hosts.yaml в папке /inventory/mycluster для описания IP-адресов всех серверов и их ролей в кластере Kubernetes.",
      "Файл будет такого вида:",
      "В файле выше мы обозначили, что на мастер-ноде устанавливаются компоненты для control-panel и ETCD запускается также на нем. ETCD — это хранилище, где Kubernetes держит все настройки и данные о работе кластера.",
      "Далее отредактируем файл all.yml в папке group_vars/all, где укажем NTP и DNS upstream-серверы для корректной работы кластера.",
      "В этом файле можно поменять настройки перед установкой (зависит от требований к кластеру), например, включить NTP и настроить использование DNS-серверов Selectel:",
      "Обратите внимание: в качестве upstream DNS серверов указываются серверы Selectel. Их IP-адреса можно найтив документации.",
      "Опционально.Вы можете внести некоторые изменения еще в файлы:",
      "В файле k8s-cluster.yaml можно указать, например, желаемый CNI (Container Network Interface), сеть для подов, а также включить настройку компонентов для PV (Persistent Volumes) и многое другое.",
      "В файле addons.yaml прописано включение ArgoCD, Helm, Ingress Nginx и другое. Изменять настройки по умолчанию мы, конечно, не будем.",
      "Далее на ВМ с Ansible выходим в папку /kubespray, из которой запустим уже непосредственно создание кластера через ansible-playbook:",
      "Ждем около 15-20 минут. И проверяем состояние нод, например с мастер-ноды, специальной kubectl-командой. Ноды должны быть в статусе Ready:",
      "В рамках данной статьи развернем кластер VictoriaMetrics, состоящей из одной виртуальной машины.",
      "Почему VictoriaMetrics?Это легковесная, но мощная альтернатива Prometheus для хранения метрик, с низким потреблением ресурсов и полной совместимостью с PromQL. Вместе они позволяют быстро развернуть кластер с эффективной системой мониторинга, подходящей как для тестов, так и для продакшена.",
      "1. Создаем новую ВМ. Конфигурация в тестовой среде следующая: 1 vCPU, 2 ГБ RAM, 10 ГБ универсального SSD-диска. Конфигурация сервера с VictoriaMetrics зависит от того, сколько метрик вы будете собирать.",
      "2. Для упрощения инфраструктуры и настройки сейчас помещаем виртуальную машину в ту же приватную подсеть, где у нас располагается кластер, в нашем случае — в сеть kubernetes-cluster.",
      "3. Выполняем на новой виртуальной машине команды:",
      "4. Далее ищем желаемый релизна ресурсеи, например, если выбираем версию 1.120.0, то устанавливаем такой командой:",
      "Версия VictoriaMetrics может меняться — стоит проверить актуальный стабильный релизна GitHub.",
      "5. Следующим шагом создаем и настраиваем systemd-юнит:",
      "5. Проверяем доступность ноды с VictoriaMetrics, например, с воркер-ноды кластера — и готово.",
      "VictoriaMetrics по умолчанию слушает запросы на порту 8428. Если вы захотите настроить файрвол, оставьте доступ для подключения по TCP по этому порту к ВМ.",
      "Теперь у вас есть работающий Kubernetes-кластер и VictoriaMetrics для сбора метрик. В следующих статьях мы расскажем, как настроить сбор метрик с помощью VMAgent и обеспечить безопасность инфраструктуры.",
      "Обратите внимание: текущая конфигурация пока не включает важные элементы безопасности. Мы не настраивали файрвол, не ограничивали доступ к узлам и не конфигурировали безопасное взаимодействие между компонентами. Эти аспекты критически важны для продакшен-среды, поэтому мы обязательно рассмотрим их в следующих публикациях, где покажем комплексный подход к защите кластера и системы мониторинга."
    ]
  }
}